@article{keyA,
  author = {Author A},
  title = {Title A},
  journal = {Journal A},
  year = {2021}
}
@book{keyB,
  author = {Author B},
  title = {Title B},
  year = {2020},
  publisher = {Publisher B}
}
@inproceedings{keyC,
  author = {Author C},
  title = {Title C},
  booktitle = {Conference C},
  year = {2022}
}

@misc{wangCCExpertAdvancingMLLM2024,
	title = {{CCExpert}: {Advancing} {MLLM} {Capability} in {Remote} {Sensing} {Change} {Captioning} with {Difference}-{Aware} {Integration} and a {Foundational} {Dataset}},
	shorttitle = {{CCExpert}},
	url = {http://arxiv.org/abs/2411.11360},
	doi = {10.48550/arXiv.2411.11360},
    publisher = {arXiv},
	abstract = {Remote Sensing Image Change Captioning (RSICC) aims to generate natural language descriptions of surface changes between multi-temporal remote sensing images, detailing the categories, locations, and dynamics of changed objects (e.g., additions or disappearances). Many current methods attempt to leverage the long-sequence understanding and reasoning capabilities of multimodal large language models (MLLMs) for this task. However, without comprehensive data support, these approaches often alter the essential feature transmission pathways of MLLMs, disrupting the intrinsic knowledge within the models and limiting their potential in RSICC. In this paper, we propose a novel model, CCExpert, based on a new, advanced multimodal large model framework. Firstly, we design a difference-aware integration module to capture multi-scale differences between bi-temporal images and incorporate them into the original image context, thereby enhancing the signal-to-noise ratio of differential features. Secondly, we constructed a high-quality, diversified dataset called CC-Foundation, containing 200,000 image pairs and 1.2 million captions, to provide substantial data support for continue pretraining in this domain. Lastly, we employed a three-stage progressive training process to ensure the deep integration of the difference-aware integration module with the pretrained MLLM. CCExpert achieved a notable performance of \$S{\textasciicircum}*\_m=81.80\$ on the LEVIR-CC benchmark, significantly surpassing previous state-of-the-art methods. The code and part of the dataset will soon be open-sourced at https://github.com/Meize0729/CCExpert.},
	urldate = {2025-03-25},
	publisher = {arXiv},
	author = {Wang, Zhiming and Wang, Mingze and Xu, Sheng and Li, Yanjing and Zhang, Baochang},
	month = nov,
	year = {2024},
	note = {arXiv:2411.11360 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\Zotero\\storage(sakura)\\storage\\TMLJSWF2\\Wang et al. - 2024 - CCExpert Advancing MLLM Capability in Remote Sensing Change Captioning with Difference-Aware Integr.pdf:application/pdf},
}

@misc{wangQwen2VLEnhancingVisionLanguage2024a,
  title = {Qwen2-{{VL}}: {{Enhancing Vision-Language Model}}'s {{Perception}} of the {{World}} at {{Any Resolution}}},
  shorttitle = {Qwen2-{{VL}}},
    publisher = {arXiv},
  author = {Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  year = {2024},
  month = sep,
  number = {arXiv:2409.12191},
  eprint = {2409.12191},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2409.12191},
  urldate = {2024-09-20},
  abstract = {We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at {\textbackslash}url\{https://github.com/QwenLM/Qwen2-VL\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D:\ZoteroLib\storage\QHP2ZDUV\Wang et al. - 2024 - Qwen2-VL Enhancing Vision-Language Model's Perception of the World at Any Resolution.pdf}
}
@article{yinSurveyMultimodalLarge2024a,
  title = {A Survey on Multimodal Large Language Models},
  author = {Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
  year = {2024},
  month = dec,
  journal = {National Science Review},
  volume = {11},
  number = {12},
  pages = {nwae403},
  issn = {2095-5138},
  doi = {10.1093/nsr/nwae403},
  urldate = {2025-01-04},
  abstract = {Recently, the multimodal large language model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful large language models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of the MLLM, such as writing stories based on images and optical character recognition--free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even outperform GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First, we present the basic formulation of the MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages and scenarios. We continue with multimodal hallucination and extended techniques, including multimodal in-context learning, multimodal chain of thought and LLM-aided visual reasoning. To conclude the paper, we discuss existing challenges and point out promising research directions.},
  file = {D:\ZoteroLib\storage\E9C6A842\Yin et al. - 2024 - A survey on multimodal large language models.pdf}
}

@inproceedings{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = jul,
  pages = {8748--8763},
  publisher = {ICML},
  issn = {2640-3498},
  urldate = {2024-07-07},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
  langid = {english},
  annotation = {CCF: A},
  file = {D\:\\ZoteroLib\\storage\\HKNJNPGP\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf;D\:\\ZoteroLib\\storage\\MRSLVU8U\\Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is All You Need},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  series = {{{NIPS}}'17},
  pages = {6000--6010},
  publisher = {NeurIPS},
  address = {Red Hook, NY, USA},
  urldate = {2024-07-01},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  isbn = {978-1-5108-6096-4},
  file = {D:\ZoteroLib\storage\THUFLBGR\Vaswani et al_2017_Attention is all you need.pdf}
}
@inproceedings{alayracFlamingoVisualLanguage2022,
  title = {Flamingo: A {{Visual Language Model}} for {{Few-Shot Learning}}},
  shorttitle = {Flamingo},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob L. and Borgeaud, Sebastian and Brock, Andy and Nematzadeh, Aida and Sharifzadeh, Sahand and Bi{\'n}kowski, Miko{\l}aj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Kar{\'e}n},
  year = {2022},
  month = dec,
  volume = {35},
  publisher = {NeurIPS},
  pages = {23716--23736},
  urldate = {2024-07-20},
  langid = {english},
  annotation = {CCF: A},
  file = {D\:\\ZoteroLib\\storage\\3EKMAJJQ\\Alayrac et al_2022_Flamingo.pdf;D\:\\ZoteroLib\\storage\\TCQT2JNF\\NeurIPS-2022-flamingo-a-visual-language-model-for-few-shot-learning-Supplemental-Conference.pdf}
}

@inproceedings{daiInstructBLIPGeneralpurposeVisionLanguage2023a,
  title = {{{InstructBLIP}}: {{Towards General-purpose Vision-Language Models}} with {{Instruction Tuning}}},
  shorttitle = {{{InstructBLIP}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N. and Hoi, Steven},
  year = {2023},
  month = dec,
  volume = {36},
  publisher = {NeurIPS},
  pages = {49250--49267},
  urldate = {2024-08-25},
  langid = {english},
  file = {D\:\\ZoteroLib\\storage\\78PWR59K\\NeurIPS-2023-instructblip-towards-general-purpose-vision-language-models-with-instruction-tuning-Supplemental-Conference.pdf;D\:\\ZoteroLib\\storage\\VS8LWHZI\\Dai et al. - 2023 - InstructBLIP Towards General-purpose Vision-Language Models with Instruction Tuning.pdf}
}

@inproceedings{liuVisualInstructionTuning2023,
  title = {Visual {{Instruction Tuning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  year = {2023},
  month = dec,
  volume = {36},
  publisher = {NeurIPS},
  pages = {34892--34916},
  urldate = {2024-06-26},
  langid = {english},
  annotation = {CCF: A},
  file = {D:\ZoteroLib\storage\HDRSMRXL\Liu et al_2023_Visual Instruction Tuning.pdf}
}

@article{luExploringModelsData2018a,
	title = {Exploring {Models} and {Data} for {Remote} {Sensing} {Image} {Caption} {Generation}},
	volume = {56},
	issn = {1558-0644},
	url = {https://ieeexplore.ieee.org/abstract/document/8240966},
	doi = {10.1109/TGRS.2017.2776321},
	abstract = {Inspired by recent development of artificial satellite, remote sensing images have attracted extensive attention. Recently, notable progress has been made in scene classification and target detection. However, it is still not clear how to describe the remote sensing image content with accurate and concise sentences. In this paper, we investigate to describe the remote sensing images with accurate and flexible sentences. First, some annotated instructions are presented to better describe the remote sensing images considering the special characteristics of remote sensing images. Second, in order to exhaustively exploit the contents of remote sensing images, a large-scale aerial image data set is constructed for remote sensing image caption. Finally, a comprehensive review is presented on the proposed data set to fully advance the task of remote sensing caption. Extensive experiments on the proposed data set demonstrate that the content of the remote sensing image can be completely described by generating language descriptions. The data set is available at https://github.com/201528014227051/RSICD\_optimal.},
	number = {4},
	urldate = {2024-09-09},
	journal = {TGRS},
	author = {Lu, Xiaoqiang and Wang, Binqiang and Zheng, Xiangtao and Li, Xuelong},
	month = apr,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	keywords = {Computer vision, Feature extraction, Semantics, Remote sensing, remote sensing image, Recurrent neural networks, Image representation, Image captioning, semantic understanding},
	pages = {2183--2195},
	file = {IEEE Xplore Abstract Record:D\:\\Zotero\\storage(sakura)\\storage\\EY2UQVWV\\8240966.html:text/html;IEEE Xplore Full Text PDF:D\:\\Zotero\\storage(sakura)\\storage\\H67238VU\\Lu et al. - 2018 - Exploring Models and Data for Remote Sensing Image Caption Generation.pdf:application/pdf},
}

@misc{liuRemoteSensingTemporal2024,
	title = {Remote {Sensing} {Temporal} {Vision}-{Language} {Models}: {A} {Comprehensive} {Survey}},
	shorttitle = {Remote {Sensing} {Temporal} {Vision}-{Language} {Models}},
	url = {http://arxiv.org/abs/2412.02573},
    publisher = {arXiv},
	doi = {10.48550/arXiv.2412.02573},
	abstract = {Temporal image analysis in remote sensing has traditionally centered on change detection, which identifies regions of change between images captured at different times. However, change detection remains limited by its focus on visual-level interpretation, often lacking contextual or descriptive information. The rise of Vision-Language Models (VLMs) has introduced a new dimension to remote sensing temporal image analysis by integrating visual information with natural language, creating an avenue for advanced interpretation of temporal image changes. Remote Sensing Temporal VLMs (RSTVLMs) allow for dynamic interactions, generating descriptive captions, answering questions, and providing a richer semantic understanding of temporal images. This temporal vision-language capability is particularly valuable for complex remote sensing applications, where higher-level insights are crucial. This paper comprehensively reviews the progress of RSTVLM research, with a focus on the latest VLM applications for temporal image analysis. We categorize and discuss core methodologies, datasets, and metrics, highlight recent advances in temporal vision-language tasks, and outline key challenges and future directions for research in this emerging field. This survey fills a critical gap in the literature by providing an integrated overview of RSTVLM, offering a foundation for further advancements in remote sensing temporal image understanding. We will keep tracing related works at {\textbackslash}url\{https://github.com/Chen-Yang-Liu/Awesome-RS-Temporal-VLM\}},
	urldate = {2025-03-25},
	author = {Liu, Chenyang and Zhang, Jiafan and Chen, Keyan and Wang, Man and Zou, Zhengxia and Shi, Zhenwei},
	month = dec,
	year = {2024},
	note = {arXiv:2412.02573 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Liu et al. - 2024 - Remote Sensing Temporal Vision-Language Models A Comprehensive Survey.pdf:D\:\\Zotero\\storage(sakura)\\storage\\MZG65X4P\\Liu et al. - 2024 - Remote Sensing Temporal Vision-Language Models A Comprehensive Survey.pdf:application/pdf},
}

@article{huRSGPTRemoteSensing2025,
	title = {{RSGPT}: {A} remote sensing vision language model and benchmark},
	volume = {224},
	issn = {0924-2716},
	shorttitle = {{RSGPT}},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271625001352},
	doi = {10.1016/j.isprsjprs.2025.03.028},
	abstract = {The emergence of large-scale Large Language Models (LLMs), with GPT-4 as a prominent example, has significantly propelled the rapid advancement of Artificial General Intelligence (AGI) and sparked the revolution of Artificial Intelligence 2.0. In the realm of remote sensing, there is a growing interest in developing large vision language models (VLMs) specifically tailored for data analysis in this domain. However, current research predominantly revolves around visual recognition tasks, lacking comprehensive, high-quality image–text datasets that are aligned and suitable for training large VLMs, which poses significant challenges to effectively training such models for remote sensing applications. In computer vision, recent research has demonstrated that fine-tuning large vision language models on small-scale, high-quality datasets can yield impressive performance in visual and language understanding. These results are comparable to state-of-the-art VLMs trained from scratch on massive amounts of data, such as GPT-4. Inspired by this captivating idea, in this work, we build a high-quality Remote Sensing Image Captioning dataset (RSICap) that facilitates the development of large VLMs in the remote sensing field. Unlike previous remote sensing datasets that either employ model-generated captions or short descriptions, RSICap comprises 2,585 human-annotated captions with rich and high-quality information. This dataset offers detailed descriptions for each image, encompassing scene descriptions (e.g., residential area, airport, or farmland) as well as object information (e.g., color, shape, quantity, absolute position, etc.). To facilitate the evaluation of VLMs in the field of remote sensing, we also provide a benchmark evaluation dataset called RSIEval. This dataset consists of human-annotated captions and visual question–answer pairs, allowing for a comprehensive assessment of VLMs in the context of remote sensing. We are actively engaged in expanding the scale of these two datasets to cover a broader spectrum of remote sensing image understanding tasks, further enhancing their utility and applicability. Our dataset and codes will be released at https://github.com/Lavender105/RSGPT.},
	urldate = {2025-06-01},
	journal = {ISPRS},
	author = {Hu, Yuan and Yuan, Jianlong and Wen, Congcong and Lu, Xiaonan and Liu, Yu and Li, Xiang},
	month = jun,
	year = {2025},
	keywords = {Multi-modal large language model (MLLM), Remote sensing (RS), Vision language model (VLM)},
	pages = {272--286},
	file = {PDF:D\:\\Zotero\\storage(sakura)\\storage\\6Y94GBT4\\Hu et al. - 2025 - RSGPT A remote sensing vision language model and benchmark.pdf:application/pdf},
}

@inproceedings{soniEarthDialTurningMultisensory2025a,
	title = {{EarthDial}: {Turning} {Multi}-sensory {Earth} {Observations} to {Interactive} {Dialogues}},
	shorttitle = {{EarthDial}},
        publisher ={CVPR},
	url = {http://arxiv.org/abs/2412.15190},
	doi = {10.48550/arXiv.2412.15190},
	abstract = {Automated analysis of vast Earth observation data via interactive Vision-Language Models (VLMs) can unlock new opportunities for environmental monitoring, disaster response, and \{resource management\}. Existing generic VLMs do not perform well on Remote Sensing data, while the recent Geo-spatial VLMs remain restricted to a fixed resolution and few sensor modalities. In this paper, we introduce EarthDial, a conversational assistant specifically designed for Earth Observation (EO) data, transforming complex, multi-sensory Earth observations into interactive, natural language dialogues. EarthDial supports multi-spectral, multi-temporal, and multi-resolution imagery, enabling a wide range of remote sensing tasks, including classification, detection, captioning, question answering, visual reasoning, and visual grounding. To achieve this, we introduce an extensive instruction tuning dataset comprising over 11.11M instruction pairs covering RGB, Synthetic Aperture Radar (SAR), and multispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore, EarthDial handles bi-temporal and multi-temporal sequence analysis for applications like change detection. Our extensive experimental results on 44 downstream datasets demonstrate that EarthDial outperforms existing generic and domain-specific models, achieving better generalization across various EO tasks. Our source codes and pre-trained models are at https://github.com/hiyamdebary/EarthDial.},
	urldate = {2025-05-24},
	author = {Soni, Sagar and Dudhane, Akshay and Debary, Hiyam and Fiaz, Mustansar and Munir, Muhammad Akhtar and Danish, Muhammad Sohail and Fraccaro, Paolo and Watson, Campbell D. and Klein, Levente J. and Khan, Fahad Shahbaz and Khan, Salman},
	month = apr,
	year = {2025},
	note = {arXiv:2412.15190 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Soni et al. - 2025 - EarthDial Turning Multi-sensory Earth Observations to Interactive Dialogues.pdf:D\:\\Zotero\\storage(sakura)\\storage\\38WS99XE\\Soni et al. - 2025 - EarthDial Turning Multi-sensory Earth Observations to Interactive Dialogues.pdf:application/pdf},
}

@inproceedings{irvinTEOChatLargeVisionLanguage2024a,
	title = {{TEOChat}: {A} {Large} {Vision}-{Language} {Assistant} for {Temporal} {Earth} {Observation} {Data}},
	shorttitle = {{TEOChat}},
	url = {https://openreview.net/forum?id=pZz0nOroGv},    
        publisher ={ICLR},
	abstract = {Large vision and language assistants have enabled new capabilities for interpreting natural images. These approaches have recently been adapted to earth observation data, but they are only able to handle single image inputs, limiting their use for many real-world tasks. In this work, we develop a new vision and language assistant called TEOChat that can engage in conversations about temporal sequences of earth observation data. To train TEOChat, we curate an instruction-following dataset composed of many single image and temporal tasks including building change and damage assessment, semantic change detection, and temporal scene classification. We show that TEOChat can perform a wide variety of spatial and temporal reasoning tasks, substantially outperforming previous vision and language assistants, and even achieving comparable or better performance than several specialist models trained to perform specific tasks. Furthermore, TEOChat achieves impressive zero-shot performance on a change detection and change question answering dataset, outperforms GPT-4o and Gemini 1.5 Pro on multiple temporal tasks, and exhibits stronger single image capabilities than a comparable single image instruction-following model on scene classification, visual question answering, and captioning. We publicly release our data, models, and code at https://github.com/ermongroup/TEOChat .},
	language = {en},
	urldate = {2025-04-03},
	author = {Irvin, Jeremy Andrew and Liu, Emily Ruoyu and Chen, Joyce C. and Dormoy, Ines and Kim, Jinyoung and Khanna, Samar and Zheng, Zhuo and Ermon, Stefano},
	month = oct,
	year = {2025},
	file = {Full Text PDF:D\:\\Zotero\\storage(sakura)\\storage\\8ZGKS9PB\\Irvin et al. - 2024 - TEOChat A Large Vision-Language Assistant for Temporal Earth Observation Data.pdf:application/pdf},
}

@misc{chenRSCC2025,
	title = {{RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events}},
year ={2025},
	author = {Chen, Zhenyuan and Wang, Chenxi and Zhang, Ningyu and Zhang, Feng},
	file = {PDF:D\:\\Zotero\\storage(sakura)\\storage\\GZ4E733V\\RSCC.pdf:application/pdf},
}
