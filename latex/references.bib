@inproceedings{kingma2013auto,
  title={Auto-Encoding Variational Bayes},
  author={Kingma, Diederik P and Welling, Max},
  publisher={ICLR},
  year={2014}
}

@inproceedings{goodfellow2014generative,
  title={Generative Adversarial Networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  publisher={NeurIPS},
  year={2014}
}

@inproceedings{sohl2015deep,
  title={Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International Conference on Machine Learning},
  publisher = {ICML},
  year={2015}
}

@inproceedings{ho2020denoising,
  title={Denoising Diffusion Probabilistic Models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  publisher={NeurIPS},
  year={2020}
}

@article{tang2024crsdiff,
  title={CRS-Diff: Controllable Remote Sensing Image Generation with Diffusion Model},
  author={Tang, Datao and Li, Wenyuan and others},
  journal={TGRS},
  year={2024}
}



@inproceedings{diffusionset2024,
	title = {{DiffusionSat}: {A} {Generative} {Foundation} {Model} for {Satellite} {Imagery}},
	shorttitle = {{DiffusionSat}},
  publisher = {ICLR},
	url = {https://openreview.net/forum?id=I5webNFDgQ},
	abstract = {Diffusion models have achieved state-of-the-art results on many modalities including images, speech, and video. However, existing models are not tailored to support remote sensing data, which is widely used in important applications including environmental monitoring and crop-yield prediction. Satellite images are significantly different from natural images -- they can be multi-spectral, irregularly sampled across time -- and existing diffusion models trained on images from the Web do not support them. Furthermore, remote sensing data is inherently spatio-temporal, requiring conditional generation tasks not supported by traditional methods based on captions or images. In this paper, we present DiffusionSat, to date the largest generative foundation model trained on a collection of publicly available large, high-resolution remote sensing datasets . As text-based captions are sparsely available for satellite images, we incorporate the associated metadata such as geolocation as conditioning information. Our method produces realistic samples and can be used to solve multiple generative tasks including temporal generation, multi-spectral superrresolution and in-painting. Our method outperforms previous state-of-the-art methods for satellite image generation and is the first large-scale \_generative\_ foundation model for satellite imagery. The project website can be found here: https://samar-khanna.github.io/DiffusionSat/},
	language = {en},
	urldate = {2024-08-30},
	author = {Khanna, Samar and Liu, Patrick and Zhou, Linqi and Meng, Chenlin and Rombach, Robin and Burke, Marshall and Lobell, David B. and Ermon, Stefano},
	month = oct,
	year = {2024},
	file = {Full Text PDF:D\:\\Zotero\\storage(sakura)\\storage\\DXUAT5XH\\Khanna et al. - 2023 - DiffusionSat A Generative Foundation Model for Satellite Imagery.pdf:application/pdf},
}



@misc{CVPR2023Tutorial,
	title = {{CVPR} 2023 {Tutorial} {Denoising} {Diffusion}-based {Generative} {Modeling}: {Foundations} and {Applications}},
	author={Vahdat，Arash and Song, Jiaming  and  Meng, Chenlin},
  shorttitle = {Denoising {Diffusion}-based {Generative} {Modeling}},
	url = {https://cvpr2023-tutorial-diffusion-models.github.io},
	abstract = {Tutorial in Conjunction with CVPR 2023},
  year={2023},
	urldate = {2024-10-16},
	file = {cvpr2023-diffusion-tutorial-part-1:D\:\\Zotero\\storage(sakura)\\storage\\FLNG7CND\\cvpr2023-diffusion-tutorial-part-1.pdf:application/pdf;cvpr2023-diffusion-tutorial-part-2:D\:\\Zotero\\storage(sakura)\\storage\\MCPB39KM\\cvpr2023-diffusion-tutorial-part-2.pdf:application/pdf;cvpr2023-diffusion-tutorial-part-3:D\:\\Zotero\\storage(sakura)\\storage\\ACF366JF\\cvpr2023-diffusion-tutorial-part-3.pdf:application/pdf},
}


@article{text2earth2025,
	title = {{Text2Earth}: {Unlocking} text-driven remote sensing image generation with a global-scale dataset and a foundation model},
	issn = {2168-6831},
	shorttitle = {{Text2Earth}},
	url = {https://ieeexplore.ieee.org/document/10988859},
	doi = {10.1109/MGRS.2025.3560455},
	abstract = {Recently, generative foundation models (GFMs) have significantly advanced large-scale text-driven natural image generation and become a prominent research trend across various vertical domains. However, in the remote sensing field, there is still a lack of research on large-scale text-to-image (text2image) generation technology. Existing remote sensing image‒text datasets are small in scale and confined to specific geographic areas and scene types. Besides, existing text2image methods have struggled to achieve global-scale, multiresolution controllability, and unbounded image generation. To address these challenges, this article presents two key contributions: the Git-10M dataset and the Text2Earth foundation model. Git-10M is a global-scale image‒text dataset consisting of 10.5 million image‒text pairs, five times larger than the previous largest one. The dataset covers a wide range of geographic scenes and contains essential geospatial metadata, significantly surpassing existing datasets in both size and diversity. Building on Git-10M, we propose Text2Earth, a 1.3 billion-parameter GFM based on the diffusion framework to model global-scale remote sensing scenes. Text2Earth integrates a resolution guidance mechanism, enabling users to specify image resolutions. A dynamic condition adaptation (DCA) strategy is proposed for training and inference to improve image generation quality. Text2Earth not only excels in zero-shot text2image generation but also demonstrates robust generalization and flexibility across multiple tasks, including unbounded scene construction, image editing, and cross-modal image generation. This robust capability surpasses previous models restricted to basic fixed sizes and limited scene types. On the previous text2image benchmark dataset, Text2Earth outperforms previous models, with a significantly improved +26.23 Fréchet inception distance (FID) score and +20.95\% zero-shot classification overall accuracy (Cls-OA) metric. Our project page is https://chen-yang-liu.github.io/Text2Earth/.},
	urldate = {2025-05-10},
	journal = {GRSM},
	author = {Liu, Chenyang and Chen, Keyan and Zhao, Rui and Zou, Zhengxia and Shi, Zhenwei},
	year = {2025},
	keywords = {Diffusion models, Foundation models, Image resolution, Image synthesis, Metadata, Noise reduction, Remote sensing, Spatial resolution, Training, Visualization},
	pages = {2--23},
	file = {Full Text PDF:D\:\\Zotero\\storage(sakura)\\storage\\CV9HYHCJ\\Liu et al. - 2025 - Text2Earth Unlocking text-driven remote sensing image generation with a global-scale dataset and a.pdf:application/pdf},
}


@inproceedings{cherti2023synthetic,
  title={Is Synthetic Data from Generative Models Ready for Image Recognition?},
  author={Cherti, Mehdi and others},
  publisher={ICLR},
  year={2023}
}

@misc{dengPPTAdvancedNueralNetwork2024,
  author       = {Zhijie Deng},
  title        = {{[CS7352]} Advanced Neural Network Theory and Application},
  year         = {2024},
  note         = {Qing Yuan Research Institute, Shanghai Jiao Tong University},
  url          = {https://thudzj.github.io/},
  language     = {en}
}

@inproceedings{Chai2025DistillationSupervisedCL,
  title={Distillation-Supervised Convolutional Low-Rank Adaptation for Efficient Image Super-Resolution},
  author={Xinning Chai and Yao Zhang and Yuxuan Zhang and Zhengxue Cheng and Yingsheng Qin and Yucai Yang and Li Song},
  year={2025},
  publisher = {CVPRW 2025},
  url={https://api.semanticscholar.org/CorpusID:277787382}
}