@inproceedings{kingma2013auto,
  title={Auto-Encoding Variational Bayes},
  author={Kingma, Diederik P and Welling, Max},
  publisher={ICLR},
  year={2014}
}

@inproceedings{goodfellow2014generative,
  title={Generative Adversarial Networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  publisher={NeurIPS},
  year={2014}
}

@inproceedings{sohl2015deep,
  title={Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International Conference on Machine Learning},
  publisher = {ICML},
  year={2015}
}

@inproceedings{ho2020denoising,
  title={Denoising Diffusion Probabilistic Models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  publisher={NeurIPS},
  year={2020}
}

@article{tang2024crsdiff,
  title={CRS-Diff: Controllable Remote Sensing Image Generation with Diffusion Model},
  author={Tang, Datao and Li, Wenyuan and others},
  journal={TGRS},
  year={2024}
}



@inproceedings{diffusionset2024,
	title = {{DiffusionSat}: {A} {Generative} {Foundation} {Model} for {Satellite} {Imagery}},
	shorttitle = {{DiffusionSat}},
  publisher = {ICLR},
	url = {https://openreview.net/forum?id=I5webNFDgQ},
	abstract = {Diffusion models have achieved state-of-the-art results on many modalities including images, speech, and video. However, existing models are not tailored to support remote sensing data, which is widely used in important applications including environmental monitoring and crop-yield prediction. Satellite images are significantly different from natural images -- they can be multi-spectral, irregularly sampled across time -- and existing diffusion models trained on images from the Web do not support them. Furthermore, remote sensing data is inherently spatio-temporal, requiring conditional generation tasks not supported by traditional methods based on captions or images. In this paper, we present DiffusionSat, to date the largest generative foundation model trained on a collection of publicly available large, high-resolution remote sensing datasets . As text-based captions are sparsely available for satellite images, we incorporate the associated metadata such as geolocation as conditioning information. Our method produces realistic samples and can be used to solve multiple generative tasks including temporal generation, multi-spectral superrresolution and in-painting. Our method outperforms previous state-of-the-art methods for satellite image generation and is the first large-scale \_generative\_ foundation model for satellite imagery. The project website can be found here: https://samar-khanna.github.io/DiffusionSat/},
	language = {en},
	urldate = {2024-08-30},
	author = {Khanna, Samar and Liu, Patrick and Zhou, Linqi and Meng, Chenlin and Rombach, Robin and Burke, Marshall and Lobell, David B. and Ermon, Stefano},
	month = oct,
	year = {2024},
	file = {Full Text PDF:D\:\\Zotero\\storage(sakura)\\storage\\DXUAT5XH\\Khanna et al. - 2023 - DiffusionSat A Generative Foundation Model for Satellite Imagery.pdf:application/pdf},
}



@misc{CVPR2023Tutorial,
	title = {{CVPR} 2023 {Tutorial} {Denoising} {Diffusion}-based {Generative} {Modeling}: {Foundations} and {Applications}},
	author={Vahdat，Arash and Song, Jiaming  and  Meng, Chenlin},
  shorttitle = {Denoising {Diffusion}-based {Generative} {Modeling}},
	url = {https://cvpr2023-tutorial-diffusion-models.github.io},
	abstract = {Tutorial in Conjunction with CVPR 2023},
  year={2023},
	urldate = {2024-10-16},
	file = {cvpr2023-diffusion-tutorial-part-1:D\:\\Zotero\\storage(sakura)\\storage\\FLNG7CND\\cvpr2023-diffusion-tutorial-part-1.pdf:application/pdf;cvpr2023-diffusion-tutorial-part-2:D\:\\Zotero\\storage(sakura)\\storage\\MCPB39KM\\cvpr2023-diffusion-tutorial-part-2.pdf:application/pdf;cvpr2023-diffusion-tutorial-part-3:D\:\\Zotero\\storage(sakura)\\storage\\ACF366JF\\cvpr2023-diffusion-tutorial-part-3.pdf:application/pdf},
}


@article{text2earth2025,
	title = {{Text2Earth}: {Unlocking} text-driven remote sensing image generation with a global-scale dataset and a foundation model},
	issn = {2168-6831},
	shorttitle = {{Text2Earth}},
	url = {https://ieeexplore.ieee.org/document/10988859},
	doi = {10.1109/MGRS.2025.3560455},
	abstract = {Recently, generative foundation models (GFMs) have significantly advanced large-scale text-driven natural image generation and become a prominent research trend across various vertical domains. However, in the remote sensing field, there is still a lack of research on large-scale text-to-image (text2image) generation technology. Existing remote sensing image‒text datasets are small in scale and confined to specific geographic areas and scene types. Besides, existing text2image methods have struggled to achieve global-scale, multiresolution controllability, and unbounded image generation. To address these challenges, this article presents two key contributions: the Git-10M dataset and the Text2Earth foundation model. Git-10M is a global-scale image‒text dataset consisting of 10.5 million image‒text pairs, five times larger than the previous largest one. The dataset covers a wide range of geographic scenes and contains essential geospatial metadata, significantly surpassing existing datasets in both size and diversity. Building on Git-10M, we propose Text2Earth, a 1.3 billion-parameter GFM based on the diffusion framework to model global-scale remote sensing scenes. Text2Earth integrates a resolution guidance mechanism, enabling users to specify image resolutions. A dynamic condition adaptation (DCA) strategy is proposed for training and inference to improve image generation quality. Text2Earth not only excels in zero-shot text2image generation but also demonstrates robust generalization and flexibility across multiple tasks, including unbounded scene construction, image editing, and cross-modal image generation. This robust capability surpasses previous models restricted to basic fixed sizes and limited scene types. On the previous text2image benchmark dataset, Text2Earth outperforms previous models, with a significantly improved +26.23 Fréchet inception distance (FID) score and +20.95\% zero-shot classification overall accuracy (Cls-OA) metric. Our project page is https://chen-yang-liu.github.io/Text2Earth/.},
	urldate = {2025-05-10},
	journal = {GRSM},
	author = {Liu, Chenyang and Chen, Keyan and Zhao, Rui and Zou, Zhengxia and Shi, Zhenwei},
	year = {2025},
	keywords = {Diffusion models, Foundation models, Image resolution, Image synthesis, Metadata, Noise reduction, Remote sensing, Spatial resolution, Training, Visualization},
	pages = {2--23},
	file = {Full Text PDF:D\:\\Zotero\\storage(sakura)\\storage\\CV9HYHCJ\\Liu et al. - 2025 - Text2Earth Unlocking text-driven remote sensing image generation with a global-scale dataset and a.pdf:application/pdf},
}

@inproceedings{tokerSatSynthAugmentingImageMask2024,
  title = {{{SatSynth}}: {{Augmenting Image-Mask Pairs}} through {{Diffusion Models}} for {{Aerial Semantic Segmentation}}},
  shorttitle = {{{SatSynth}}},
  publisher={CVPR},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Toker, Aysim and Eisenberger, Marvin and Cremers, Daniel and {Leal-Taix{\'e}}, Laura},
  year = {2024},
  pages = {27695--27705},
  urldate = {2024-11-21},
  file = {D\:\\ZoteroLib\\storage\\LTNUQYNE\\Toker_SatSynth_Augmenting_Image-Mask_CVPR_2024_supplemental.pdf;D\:\\ZoteroLib\\storage\\Z83J4BG2\\Toker et al. - 2024 - SatSynth Augmenting Image-Mask Pairs through Diffusion Models for Aerial Semantic Segmentation.pdf}
}


@inproceedings{heSYNTHETICDATAGENERATIVE2022,
  title = {Is synthetic data from generative models ready for image recognition?},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {He, Ruifei and Sun, Shuyang and Yu, Xin and Xue, Chuhui and Zhang, Wenqing and Torr, Philip and Bai, Song and Qi, Xiaojuan},
  year = {2023},
  publisher = {ICLR},
  month = sep,
  urldate = {2024-09-01},
  abstract = {Recent text-to-image generation models have shown promising results in generating high-fidelity photo-realistic images. Though the results are astonishing to human eyes, how applicable these generated images are for recognition tasks remains under-explored. In this work, we extensively study whether and how synthetic images generated from state-of-the-art text-to-image generation models can be used for image recognition tasks, and focus on two perspectives: synthetic data for improving classification models in the data-scare settings (i.e. zero-shot and few-shot), and synthetic data for large-scale model pre-training for transfer learning. We showcase the powerfulness and shortcomings of synthetic data from existing generative models, and propose strategies for better applying synthetic data for recognition tasks. Code: https://github.com/CVMI-Lab/SyntheticData.},
  file = {D\:\\ZoteroLib\\storage\\AKJ2342C\\He et al. - 2022 - IS SYNTHETIC DATA FROM GENERATIVE MODELS READY FOR IMAGE RECOGNITION.pdf;D\:\\ZoteroLib\\storage\\QXK48PZH\\id220_supp_final.pdf}
}


@misc{dengPPTAdvancedNueralNetwork2024,
  author       = {Deng, Zhijie},
  title        = {{[CS7352]} Advanced Neural Network Theory and Application},
  publisher     ={SJTU Spring},
  year         = {2024},
  note         = {Qing Yuan Research Institute, Shanghai Jiao Tong University},
  url          = {https://thudzj.github.io/},
  language     = {en}
}

@inproceedings{Chai2025DistillationSupervisedCL,
  title={Distillation-Supervised Convolutional Low-Rank Adaptation for Efficient Image Super-Resolution},
  author={Xinning Chai and Yao Zhang and Yuxuan Zhang and Zhengxue Cheng and Yingsheng Qin and Yucai Yang and Li Song},
  year={2025},
  publisher = {CVPRW 2025},
  url={https://api.semanticscholar.org/CorpusID:277787382}
}

@article{liuRemoteCLIPVisionLanguage2024,
  title = {RemoteCLIP: A Vision Language Foundation Model for Remote Sensing},
  shorttitle = {RemoteCLIP},
  author = {Liu, Fan and Chen, Delong and Guan, Zhangqingyun and Zhou, Xiaocong and Zhu, Jiale and Ye, Qiaolin and Fu, Liyong and Zhou, Jun},
  year = {2024},
  journal = {TGRS},
  volume = {62},
  pages = {1--16},
  issn = {1558-0644},
  doi = {10.1109/TGRS.2024.3390838},
  urldate = {2024-08-13}
}



@inproceedings{radfordLearningTransferableVisual2021,
  title = {Learning Transferable Visual Models From Natural Language Supervision},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  pages = {8748--8763},
  publisher = {ICML},
  issn = {2640-3498},
  urldate = {2024-07-07}
}

@inproceedings{wangRealESRGANTrainingRealWorld2021b,
  title = {Real-ESRGAN: Training Real-World Blind Super-Resolution With Pure Synthetic Data},
  shorttitle = {Real-ESRGAN},
  author = {Wang, Xintao and Xie, Liangbin and Dong, Chao and Shan, Ying},
  year = {2021},
  pages = {1905--1914},
  publisher = {ICCV},
  urldate = {2025-06-06}
}

@misc{chenFaithDiffUnleashingDiffusion2024,
  title = {{{FaithDiff}}: {{Unleashing Diffusion Priors}} for {{Faithful Image Super-resolution}}},
  shorttitle = {{{FaithDiff}}},
  author = {Chen, Junyang and Pan, Jinshan and Dong, Jiangxin},
  year = {2025},
  month = nov,
  number = {arXiv:2411.18824},
  eprint = {2411.18824},
  primaryclass = {cs},
  publisher = {CVPR},
  doi = {10.48550/arXiv.2411.18824},
  urldate = {2025-06-06},
  abstract = {Faithful image super-resolution (SR) not only needs to recover images that appear realistic, similar to image generation tasks, but also requires that the restored images maintain fidelity and structural consistency with the input. To this end, we propose a simple and effective method, named FaithDiff, to fully harness the impressive power of latent diffusion models (LDMs) for faithful image SR. In contrast to existing diffusion-based SR methods that freeze the diffusion model pre-trained on high-quality images, we propose to unleash the diffusion prior to identify useful information and recover faithful structures. As there exists a significant gap between the features of degraded inputs and the noisy latent from the diffusion model, we then develop an effective alignment module to explore useful features from degraded inputs to align well with the diffusion process. Considering the indispensable roles and interplay of the encoder and diffusion model in LDMs, we jointly fine-tune them in a unified optimization framework, facilitating the encoder to extract useful features that coincide with diffusion process. Extensive experimental results demonstrate that FaithDiff outperforms state-of-the-art methods, providing high-quality and faithful SR results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\ZoteroLib\\storage\\XZ6C8G5Q\\Chen et al. - 2024 - FaithDiff Unleashing Diffusion Priors for Faithful Image Super-resolution.pdf;D\:\\ZoteroLib\\storage\\E4M9S6T7\\2411.html}
}

@article{yueEfficientDiffusionModel2025,
  title = {Efficient {{Diffusion Model}} for {{Image Restoration}} by {{Residual Shifting}}},
  author = {Yue, Zongsheng and Wang, Jianyi and Loy, Chen Change},
  year = {2025},
  month = jan,
  journal = {TPAMI},
  volume = {47},
  number = {1},
  pages = {116--130},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2024.3461721},
  urldate = {2025-06-06},
  abstract = {While diffusion-based image restoration (IR) methods have achieved remarkable success, they are still limited by the low inference speed attributed to the necessity of executing hundreds or even thousands of sampling steps. Existing acceleration sampling techniques, though seeking to expedite the process, inevitably sacrifice performance to some extent, resulting in over-blurry restored outcomes. To address this issue, this study proposes a novel and efficient diffusion model for IR that significantly reduces the required number of diffusion steps. Our method avoids the need for post-acceleration during inference, thereby avoiding the associated performance deterioration. Specifically, our proposed method establishes a Markov chain that facilitates the transitions between the high-quality and low-quality images by shifting their residuals, substantially improving the transition efficiency. A carefully formulated noise schedule is devised to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experimental evaluations demonstrate that the proposed method achieves superior or comparable performance to current state-of-the-art methods on four classical IR tasks, namely image super-resolution, image inpainting, blind face restoration, and image deblurring, even only with four sampling steps.},
  keywords = {Degradation,Diffusion models,Face restoration,image inpainting,Image restoration,image super-resolution,Image synthesis,Markov chain,Noise,noise schedule,Schedules,Superresolution},
  file = {D:\ZoteroLib\storage\KMMSW6PS\Yue et al. - 2025 - Efficient Diffusion Model for Image Restoration by Residual Shifting.pdf}
}

@article{wangExploitingDiffusionPrior2024,
  title = {Exploiting {{Diffusion Prior}} for {{Real-World Image Super-Resolution}}},
  author = {Wang, Jianyi and Yue, Zongsheng and Zhou, Shangchen and Chan, Kelvin C. K. and Loy, Chen Change},
  year = {2024},
  month = dec,
  journal = {IJCV},
  volume = {132},
  number = {12},
  pages = {5929--5949},
  issn = {1573-1405},
  doi = {10.1007/s11263-024-02168-7},
  urldate = {2025-06-06},
  abstract = {We present a novel approach to leverage prior knowledge encapsulated in pre-trained text-to-image diffusion models for blind super-resolution. Specifically, by employing our time-aware encoder, we can achieve promising restoration results without altering the pre-trained synthesis model, thereby preserving the generative prior and minimizing training cost. To remedy the loss of fidelity caused by the inherent stochasticity of diffusion models, we employ a controllable feature wrapping module that allows users to balance quality and fidelity by simply adjusting a scalar value during the inference process. Moreover, we develop a progressive aggregation sampling strategy to overcome the fixed-size constraints of pre-trained diffusion models, enabling adaptation to resolutions of any size. A comprehensive evaluation of our method using both synthetic and real-world benchmarks demonstrates its superiority over current state-of-the-art approaches. Code and models are available at https://github.com/IceClear/StableSR.},
  langid = {english},
  keywords = {Applications of Brownian Motion and Diffusion Theory,Diffusion  Processes and Stochastic Analysis on  Manifolds,Diffusion models,Diffusion Process,Diffusion Tensor Imaging,Generative prior,Image Processing,Image restoration,Super-resolution,Super-Resolution Microscopy},
  file = {D:\ZoteroLib\storage\7PKMLKRG\Wang et al. - 2024 - Exploiting Diffusion Prior for Real-World Image Super-Resolution.pdf}
}

@article{devriesImprovedRegularizationConvolutional2017,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1708.04552},
  year={2017}
}

@inproceedings{yunCutmixRegularizationStrategy2019,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6023--6032},
  year={2019},
  publisher={ICCV}
}

@inproceedings{ghiasiSimpleCopyPasteStrong2021,
  title={Simple copy-paste is a strong data augmentation method for instance segmentation},
  author={Ghiasi, Golnaz and Cui, Yin and Srinivas, Aravind and Qian, Rui and Lin, Tsung-Yi and Cubuk, Ekin D and Le, Quoc V and Zoph, Barret},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2918--2928},
  year={2021},
  publisher={CVPR}
}

@inproceedings{waqas2019isaid,
  title={isaid: A large-scale dataset for instance segmentation in aerial images},
  author={Waqas Zamir, Syed and Arora, Aditya and Gupta, Akshita and Khan, Salman and Sun, Guolei and Shahbaz Khan, Fahad and Zhu, Fan and Shao, Ling and Xia, Gui-Song and Bai, Xiang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={28--37},
  year={2019},
  publisher = {CVPRW},
}


@article{lu2017exploring,
	title = {Exploring {Models} and {Data} for {Remote} {Sensing} {Image} {Caption} {Generation}},
	volume = {56},
	issn = {1558-0644},
	url = {https://ieeexplore.ieee.org/abstract/document/8240966},
	doi = {10.1109/TGRS.2017.2776321},
	abstract = {Inspired by recent development of artificial satellite, remote sensing images have attracted extensive attention. Recently, notable progress has been made in scene classification and target detection. However, it is still not clear how to describe the remote sensing image content with accurate and concise sentences. In this paper, we investigate to describe the remote sensing images with accurate and flexible sentences. First, some annotated instructions are presented to better describe the remote sensing images considering the special characteristics of remote sensing images. Second, in order to exhaustively exploit the contents of remote sensing images, a large-scale aerial image data set is constructed for remote sensing image caption. Finally, a comprehensive review is presented on the proposed data set to fully advance the task of remote sensing caption. Extensive experiments on the proposed data set demonstrate that the content of the remote sensing image can be completely described by generating language descriptions. The data set is available at https://github.com/201528014227051/RSICD\_optimal.},
	number = {4},
	urldate = {2024-09-09},
	journal = {TGRS},
	author = {Lu, Xiaoqiang and Wang, Binqiang and Zheng, Xiangtao and Li, Xuelong},
	month = apr,
	year = {2018},
	note = {RSICD},
	keywords = {Computer vision, Feature extraction, Semantics, Remote sensing, remote sensing image, Recurrent neural networks, Image representation, Image captioning, semantic understanding},
	pages = {2183--2195},
	file = {IEEE Xplore Abstract Record:D\:\\Zotero\\storage(sakura)\\storage\\EY2UQVWV\\8240966.html:text/html;IEEE Xplore Full Text PDF:D\:\\Zotero\\storage(sakura)\\storage\\H67238VU\\Lu et al. - 2018 - Exploring Models and Data for Remote Sensing Image Caption Generation.pdf:application/pdf},
}


@article{congFunctionalMapWorld2022,
	title = {Functional {Map} of the {World} - {Sentinel}-2 corresponding images},
	url = {https://purl.stanford.edu/vg497cb6002},
	doi = {10.25740/vg497cb6002},
	abstract = {The Functional Map of the World - Sentinel-2 corresponding images (fMoW-Sentinel) dataset consists of image time series collected by the Sentinel-2 satellite, corresponding to locations from the Fu...},
	language = {en},
	urldate = {2024-06-29},
	author = {Cong, Yezhen and Khanna, Samar and Meng, Chenlin and Liu, Patrick and Rozi, Erik and He, Yutong and Burke, Marshall and Lobell, David B. and Ermon, Stefano},
	year = {2022},
	file = {Snapshot:D\:\\Zotero\\storage(sakura)\\storage\\R3V2PZ43\\vg497cb6002.html:text/html},
}


@inproceedings{christieFunctionalMapWorld2018,
	title = {Functional {Map} of the {World}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Christie_Functional_Map_of_CVPR_2018_paper.html},
	urldate = {2024-06-25},
  publisher = {CVPR},
	author = {Christie, Gordon and Fendley, Neil and Wilson, James and Mukherjee, Ryan},
	year = {2018},
	note = {https://github.com/fMoW/dataset},
	pages = {6172--6180},
	file = {Christie et al_2018_Functional Map of the World.pdf:D\:\\Zotero\\storage(sakura)\\storage\\VTS3XRJS\\Christie et al_2018_Functional Map of the World.pdf:application/pdf;Christie et al. - Supplementary Material Functional Map of the Worl.pdf:D\:\\Zotero\\storage(sakura)\\storage\\39FUI7KE\\Christie et al. - Supplementary Material Functional Map of the Worl.pdf:application/pdf},
}


@article{hu2023rsgpt,
	title = {{RSGPT}: {A} remote sensing vision language model and benchmark},
	volume = {224},
	issn = {0924-2716},
	shorttitle = {{RSGPT}},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271625001352},
	doi = {10.1016/j.isprsjprs.2025.03.028},
	abstract = {The emergence of large-scale Large Language Models (LLMs), with GPT-4 as a prominent example, has significantly propelled the rapid advancement of Artificial General Intelligence (AGI) and sparked the revolution of Artificial Intelligence 2.0. In the realm of remote sensing, there is a growing interest in developing large vision language models (VLMs) specifically tailored for data analysis in this domain. However, current research predominantly revolves around visual recognition tasks, lacking comprehensive, high-quality image–text datasets that are aligned and suitable for training large VLMs, which poses significant challenges to effectively training such models for remote sensing applications. In computer vision, recent research has demonstrated that fine-tuning large vision language models on small-scale, high-quality datasets can yield impressive performance in visual and language understanding. These results are comparable to state-of-the-art VLMs trained from scratch on massive amounts of data, such as GPT-4. Inspired by this captivating idea, in this work, we build a high-quality Remote Sensing Image Captioning dataset (RSICap) that facilitates the development of large VLMs in the remote sensing field. Unlike previous remote sensing datasets that either employ model-generated captions or short descriptions, RSICap comprises 2,585 human-annotated captions with rich and high-quality information. This dataset offers detailed descriptions for each image, encompassing scene descriptions (e.g., residential area, airport, or farmland) as well as object information (e.g., color, shape, quantity, absolute position, etc.). To facilitate the evaluation of VLMs in the field of remote sensing, we also provide a benchmark evaluation dataset called RSIEval. This dataset consists of human-annotated captions and visual question–answer pairs, allowing for a comprehensive assessment of VLMs in the context of remote sensing. We are actively engaged in expanding the scale of these two datasets to cover a broader spectrum of remote sensing image understanding tasks, further enhancing their utility and applicability. Our dataset and codes will be released at https://github.com/Lavender105/RSGPT.},
	urldate = {2025-06-01},
	journal = {ISPRS},
	author = {Hu, Yuan and Yuan, Jianlong and Wen, Congcong and Lu, Xiaonan and Liu, Yu and Li, Xiang},
	month = jun,
	year = {2025},
	keywords = {Multi-modal large language model (MLLM), Remote sensing (RS), Vision language model (VLM)},
	pages = {272--286},
	file = {PDF:D\:\\Zotero\\storage(sakura)\\storage\\6Y94GBT4\\Hu et al. - 2025 - RSGPT A remote sensing vision language model and benchmark.pdf:application/pdf},
}


@inproceedings{qu2016deep,
  title={Deep semantic understanding of high resolution remote sensing image},
  author={Qu, Bo and Li, Xuelong and Tao, Dacheng and Lu, Xiaoqiang},
  booktitle={2016 International Conference on Computer, Information and Telecommunication Systems (CITS)},
  pages={1--5},
  year={2016},
  organization={IEEE},
  publisher = {CITS},
}
@article{mengConditionalDiffusionModel2024,
  title = {A {{Conditional Diffusion Model With Fast Sampling Strategy}} for {{Remote Sensing Image Super-Resolution}}},
  author = {Meng, Fanen and Chen, Yijun and Jing, Haoyu and Zhang, Laifu and Yan, Yiming and Ren, Yingchao and Wu, Sensen and Feng, Tian and Liu, Renyi and Du, Zhenhong},
  year = {2024},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {62},
  pages = {1--16},
  issn = {1558-0644},
  doi = {10.1109/TGRS.2024.3458009},
  urldate = {2025-06-09},
  abstract = {Conventional deep learning-based methods for single remote sensing image super-resolution (SRSISR) have made remarkable progress. However, the super-resolution (SR) outputs of these methods are yet to become sufficiently satisfactory in visual quality. Recent diffusion model-based generative deep learning models are capable to enhance the visual quality of output images, but this capability is limited due to their sampling efficiency. In this article, we propose FastDiffSR, an SRSISR method based on a conditional diffusion model. Specifically, we devise a novel sampling strategy to reduce the number of sampling steps required by the diffusion model while ensuring the sampling quality. Meanwhile, the residual image is adopted to reduce computational costs, demonstrating that integrating channel attention and spatial attention begets a further improvement in the visual quality of output images. Compared to the state-of-the-art (SOTA) convolutional neural network (CNN)-based, GAN-based, and Transformer-based SR methods, our FastDiffSR improves the learned perceptual image patch similarity (LPIPS) by 0.1--0.2 and achieves better visual results in some real-world scenes. Compared with existing diffusion-based SR methods, our FastDiffSR achieves significant improvements in pixel-level evaluation metric peak signal-noise ratio (PSNR) while having smaller model parameters and obtaining better SR results on Vaihingen data with faster inference time by 2.8--28 times, showing excellent generalization ability and time efficiency. Our code will be open source at https://github.com/Meng-333/FastDiffSR.},
  keywords = {Computational modeling,Conditional diffusion model,deep learning,Diffusion models,generative models,remote sensing,Remote sensing,super-resolution (SR),Superresolution,Training,Transformers,Visualization},
  file = {D:\ZoteroLib\storage\U3SEYGG7\Meng et al. - 2024 - A Conditional Diffusion Model With Fast Sampling Strategy for Remote Sensing Image Super-Resolution.pdf}
}

@inproceedings{ucm2010,
author = {Yang, Yi and Newsam, Shawn},
title = {Bag-of-visual-words and spatial extensions for land-use classification},
year = {2010},
isbn = {9781450304283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1869790.1869829},
doi = {10.1145/1869790.1869829},
abstract = {We investigate bag-of-visual-words (BOVW) approaches to land-use classification in high-resolution overhead imagery. We consider a standard non-spatial representation in which the frequencies but not the locations of quantized image features are used to discriminate between classes analogous to how words are used for text document classification without regard to their order of occurrence. We also consider two spatial extensions, the established spatial pyramid match kernel which considers the absolute spatial arrangement of the image features, as well as a novel method which we term the spatial co-occurrence kernel that considers the relative arrangement. These extensions are motivated by the importance of spatial structure in geographic data.The methods are evaluated using a large ground truth image dataset of 21 land-use classes. In addition to comparisons with standard approaches, we perform extensive evaluation of different configurations such as the size of the visual dictionaries used to derive the BOVW representations and the scale at which the spatial relationships are considered.We show that even though BOVW approaches do not necessarily perform better than the best standard approaches overall, they represent a robust alternative that is more effective for certain land-use classes. We also show that extending the BOVW approach with our proposed spatial co-occurrence kernel consistently improves performance.},
booktitle = {Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {270–279},
numpages = {10},
keywords = {bag-of-visual-words, land-use classification, local invariant features},
location = {San Jose, California},
series = {GIS '10}
}

@article{ROTTENSTEINER2014256,
title = {Results of the ISPRS benchmark on urban object detection and 3D building reconstruction},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {93},
pages = {256-271},
year = {2014},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2013.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0924271613002268},
author = {Franz Rottensteiner and Gunho Sohn and Markus Gerke and Jan Dirk Wegner and Uwe Breitkopf and Jaewook Jung},
keywords = {Automatic object extraction, 3D building reconstruction, Aerial imagery, Laser scanning, Evaluation, Benchmarking test},
abstract = {For more than two decades, many efforts have been made to develop methods for extracting urban objects from data acquired by airborne sensors. In order to make the results of such algorithms more comparable, benchmarking data sets are of paramount importance. Such a data set, consisting of airborne image and laserscanner data, has been made available to the scientific community by ISPRS WGIII/4. Researchers were encouraged to submit their results of urban object detection and 3D building reconstruction, which were evaluated based on reference data. This paper presents the outcomes of the evaluation for building detection, tree detection, and 3D building reconstruction. The results achieved by different methods are compared and analysed to identify promising strategies for automatic urban object extraction from current airborne sensor data, but also common problems of state-of-the-art methods.}
}

@inproceedings{postdam,
  title = {Fully {{Convolutional Networks}} for {{Dense Semantic Labelling}} of {{High-Resolution Aerial Imagery}}},
  author = {Sherrah, Jamie},
  year = {2016},
  month = jun,
  number = {arXiv:1606.02585},
  eprint = {1606.02585},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1606.02585},
  urldate = {2025-06-09},
  abstract = {The trend towards higher resolution remote sensing imagery facilitates a transition from land-use classification to object-level scene understanding. Rather than relying purely on spectral content, appearance-based image features come into play. In this work, deep convolutional neural networks (CNNs) are applied to semantic labelling of high-resolution remote sensing data. Recent advances in fully convolutional networks (FCNs) are adapted to overhead data and shown to be as effective as in other domains. A full-resolution labelling is inferred using a deep FCN with no downsampling, obviating the need for deconvolution or interpolation. To make better use of image features, a pre-trained CNN is fine-tuned on remote sensing data in a hybrid network context, resulting in superior results compared to a network trained from scratch. The proposed approach is applied to the problem of labelling high-resolution aerial imagery, where fine boundary detail is important. The dense labelling yields state-of-the-art accuracy for the ISPRS Vaihingen and Potsdam benchmark data sets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}
