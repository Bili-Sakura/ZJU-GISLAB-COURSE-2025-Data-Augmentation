@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  publisher = {NeurIPS},
  urldate = {2024-04-29},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {D:\ZoteroLib\storage\HGIP47U9\Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf}
}

@article{Cheng2017,
   title={Remote Sensing Image Scene Classification: Benchmark and State of the Art},
   volume={105},
   ISSN={1558-2256},
   url={http://dx.doi.org/10.1109/JPROC.2017.2675998},
   DOI={10.1109/jproc.2017.2675998},
   number={10},
   journal={Proceedings of the IEEE},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Cheng, Gong and Han, Junwei and Lu, Xiaoqiang},
   year={2017},
   month={Oct},
   pages={1865-1883}
}


@inproceedings{guptaCreatingXBDDataset2019,
  title={xBD: A Dataset for Assessing Building Damage from Satellite Imagery}, 
  author={Ritwik Gupta and Richard Hosfelt and Sandra Sajeev and Nirav Patel and Bryce Goodman and Jigar Doshi and Eric Heim and Howie Choset and Matthew Gaston},
  year={2019},
  publisher={arXiv, CVPRW},
  eprint={1911.09296},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/1911.09296}, 
}

@article{steinerHowTrainYour2022,
  title = {How to Train Your {{ViT}}? {{Data}}, {{Augmentation}}, and {{Regularization}} in {{Vision Transformers}}},
  shorttitle = {How to Train Your {{ViT}}?},
  author = {Steiner, Andreas Peter and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  year = {2022},
  month = apr,
  journal = {TMLR},
  issn = {2835-8856},
  urldate = {2025-06-10},
  abstract = {Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer's weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (``AugReg'' for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.},
  langid = {english},
  file = {D:\ZoteroLib\storage\M9MEK48J\Steiner et al. - 2022 - How to train your ViT Data, Augmentation, and Regularization in Vision Transformers.pdf}
}


@inproceedings{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  booktitle = {International {{Conference}} on {{Computer Vision}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  publisher = {ICCV},
  month = oct,
  pages = {9992--10002},
  issn = {2380-7504},
  doi = {10.1109/ICCV48922.2021.00986},
  urldate = {2024-04-15},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.},
  keywords = {Computational modeling,Computer architecture,Computer vision,Detection and localization in 2D and 3D,DNN,grouping and shape,ICCV 2024,Image segmentation,Object detection,Recognition and classification,Representation learning,Segmentation,Semantics,Visualization},
  file = {D:\ZoteroLib\storage\R39QXNUD\Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf}
}


@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  publisher = {CVPR},
  pages = {770--778},
  urldate = {2024-06-20},
  file = {D:\ZoteroLib\storage\G6388CZG\He et al_2016_Deep Residual Learning for Image Recognition.pdf}
}

@misc{devriesImprovedRegularizationConvolutional2017,
  title = {Improved {{Regularization}} of {{Convolutional Neural Networks}} with {{Cutout}}},
  author = {DeVries, Terrance and Taylor, Graham W.},
  year = {2017},
  publisher={arXiv},
  month = aug,
  urldate = {2025-06-09},
  abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56\%, 15.20\%, and 1.30\% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout},
  langid = {english},
  file = {D:\ZoteroLib\storage\3RBNHX6L\DeVries and Taylor - 2017 - Improved Regularization of Convolutional Neural Networks with Cutout.pdf}
}

@inproceedings{dosovitskiyImageWorth16x162020,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  publisher = {ICLR},
  month = oct,
  urldate = {2024-04-28},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  langid = {english},
  keywords = {DNN,Google,ViT},
  file = {D:\ZoteroLib\storage\7IWEULRX\Dosovitskiy 等 - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf}
}


@article{yuCoCaContrastiveCaptioners2022,
  title = {{{CoCa}}: {{Contrastive Captioners}} Are {{Image-Text Foundation Models}}},
  shorttitle = {{{CoCa}}},
  author = {Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  year = {2022},
  month = jul,
  journal = {TMLR},
  issn = {2835-8856},
  urldate = {2025-06-10},
  abstract = {Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3\% zero-shot top-1 accuracy, 90.6\% with a frozen encoder and learned classification head, and 91.0\% with a finetuned encoder.},
  langid = {english},
  file = {D:\ZoteroLib\storage\MG4867AG\Yu et al. - 2022 - CoCa Contrastive Captioners are Image-Text Foundation Models.pdf}
}

@inproceedings{heMaskedAutoencodersAre2022,
  title = {Masked {{Autoencoders Are Scalable Vision Learners}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2022},
  publisher ={CVPR},
  pages = {16000--16009},
  urldate = {2024-08-20},
  langid = {english},
  file = {D\:\\ZoteroLib\\storage\\87YIQASS\\He_Masked_Autoencoders_Are_CVPR_2022_supplemental.pdf;D\:\\ZoteroLib\\storage\\SHI3PRMZ\\He et al. - 2022 - Masked Autoencoders Are Scalable Vision Learners.pdf}
}


@inproceedings{ghiasiSimpleCopyPasteStrong2021a,
  title = {Simple {{Copy-Paste Is}} a {{Strong Data Augmentation Method}} for {{Instance Segmentation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ghiasi, Golnaz and Cui, Yin and Srinivas, Aravind and Qian, Rui and Lin, Tsung-Yi and Cubuk, Ekin D. and Le, Quoc V. and Zoph, Barret},
  year = {2021},
  publisher = {CVPR},
  pages = {2918--2928},
  urldate = {2025-06-09},
  langid = {english},
  file = {D\:\\ZoteroLib\\storage\\3UKMC7WH\\Ghiasi_Simple_Copy-Paste_Is_CVPR_2021_supplemental.pdf;D\:\\ZoteroLib\\storage\\S2J7U66T\\Ghiasi et al. - 2021 - Simple Copy-Paste Is a Strong Data Augmentation Method for Instance Segmentation.pdf}
}

@inproceedings{yunCutMixRegularizationStrategy2019,
  title = {{{CutMix}}: {{Regularization Strategy}} to {{Train Strong Classifiers With Localizable Features}}},
  shorttitle = {{{CutMix}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  year = {2019},
  publisher = {ICCV},
  pages = {6023--6032},
  urldate = {2025-06-06},
  file = {D\:\\ZoteroLib\\storage\\HG75M439\\Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Classifiers With Localizable Features.pdf;D\:\\ZoteroLib\\storage\\NA5HP9L4\\Yun et al. - CutMix Regularization Strategy to Train Strong Classiﬁers with Localizable Features – Supplementary.pdf}
}


@article{liuRemoteCLIPVisionLanguage2024,
  title = {{{RemoteCLIP}}: {{A Vision Language Foundation Model}} for {{Remote Sensing}}},
  shorttitle = {{{RemoteCLIP}}},
  author = {Liu, Fan and Chen, Delong and Guan, Zhangqingyun and Zhou, Xiaocong and Zhu, Jiale and Ye, Qiaolin and Fu, Liyong and Zhou, Jun},
  year = {2024},
  journal = {TGRS},
  volume = {62},
  pages = {1--16},
  issn = {1558-0644},
  doi = {10.1109/TGRS.2024.3390838},
  urldate = {2024-08-13},
  abstract = {General-purpose foundation models have led to recent breakthroughs in artificial intelligence (AI). In remote sensing, self-supervised learning (SSL) and masked image modeling (MIM) have been adopted to build foundation models. However, these models primarily learn low-level features and require annotated data for fine-tuning. Moreover, they are inapplicable for retrieval and zero-shot applications due to the lack of language understanding. To address these limitations, we propose RemoteCLIP, the first vision-language foundation model for remote sensing that aims to learn robust visual features with rich semantics and aligned text embeddings for seamless downstream application. To address the scarcity of pretraining data, we leverage data scaling which converts heterogeneous annotations into a unified image-caption data format based on box-to-caption (B2C) and mask-to-box (M2B) conversion. By further incorporating unmanned aerial vehicle (UAV) imagery, we produce a 12{\textbackslash}times larger pretraining dataset than the combination of all available datasets. RemoteCLIP can be applied to a variety of downstream tasks, including zero-shot image classification, linear probing, k-NN classification, few-shot classification, image-text retrieval, and object counting in remote sensing images. Evaluation of 16 datasets, including a newly introduced RemoteCount benchmark to test the object counting ability, shows that RemoteCLIP consistently outperforms baseline foundation models across different model scales. Impressively, RemoteCLIP beats the state-of-the-art (SOTA) method by 9.14\% mean recall on the RSITMD dataset and 8.92\% on the RSICD dataset. For zero-shot classification, our RemoteCLIP outperforms the contrastive language image pretraining (CLIP) baseline by up to 6.39\% average accuracy on 12 downstream datasets.},
  keywords = {Adaptation models,Computational modeling,Contrastive language image pretraining (CLIP),Data models,foundation model,multimodality,remote sensing,Remote sensing,Semantics,Task analysis,vision-language,Visualization},
  file = {D\:\\ZoteroLib\\storage\\7NRNHX4S\\Liu et al. - 2024 - RemoteCLIP A Vision Language Foundation Model for Remote Sensing.pdf;D\:\\ZoteroLib\\storage\\9R9DKFG5\\2306.11029v4.pdf;D\:\\ZoteroLib\\storage\\B6B2WHT3\\10504785.html}
}


@inproceedings{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = jul,
  pages = {8748--8763},
  publisher = {ICML},
  issn = {2640-3498},
  urldate = {2024-07-07},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
  langid = {english},
  annotation = {CCF: A},
  file = {D\:\\ZoteroLib\\storage\\HKNJNPGP\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf;D\:\\ZoteroLib\\storage\\MRSLVU8U\\Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf}
}
@misc{gptimage1,
  title        = {GPT-Image-1},
  author = {OpenAI},
  howpublished = {OpenAI API documentation},
  note         = {OpenAI’s newest text-to-image model},
  year         = {2025},
  url          = {https://platform.openai.com/docs/models/gpt-image-1},
}

@inproceedings{zhuSkySenseOOpenWorldRemote2025,
	title = {{SkySense}-O: Towards Open-World Remote Sensing Interpretation with Vision-Centric Visual-Language Modeling},
	publisher = {{CVPR}},
	author = {Zhu, Qi and Lao, Jiangwei and Ji, Deyi and Luo, Junwei and Wu, Kang and Zhang, Yingying and Ru, Lixiang and Wang, Jian and Chen, Jingdong and Yang, Ming and Liu, Dong and Zhao, Feng},
	date = {2025},
}

@misc{gemini2,
  title        = {Experiment with gemini 2.0 flash native image generation},
  author={Google},
  howpublished = {Google Cloud Vertex AI model card},
  year         = {2024},
  url          = {https://developers.googleblog.com/en/experiment-with-gemini-20-flash-native-image-generation},
}
@article{wang2025seedit,
  title   = {SeedEdit 3.0: Fast and High‑Quality Generative Image Editing},
  author  = {Wang, Peng and Shi, Yichun and Lian, Xiaochen and Zhai, Zhonghua and Xia, Xin and Xiao, Xuefeng and Huang, Weilin and Yang, Jianchao},
  journal = {arXiv},
  year    = {2025},
}


@misc{imagenet2010challenge,
  author = {Berg, A. and Deng, J. and Fei-Fei, L.},
  title = {Large Scale Visual Recognition Challenge 2010},
  year = {2010},
  howpublished = {\url{http://www.imagenet.org/challenges}},
  note = {Accessed: 2024-06-01}
}

@inproceedings{kingma2013auto,
  title={Auto-Encoding Variational Bayes},
  author={Kingma, Diederik P and Welling, Max},
  publisher={ICLR},
  year={2014}
}

@inproceedings{goodfellow2014generative,
  title={Generative Adversarial Networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  publisher={NeurIPS},
  year={2014}
}

@inproceedings{sohl2015deep,
  title={Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International Conference on Machine Learning},
  publisher = {ICML},
  year={2015}
}

@inproceedings{ho2020denoising,
  title={Denoising Diffusion Probabilistic Models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  publisher={NeurIPS},
  year={2020}
}

@article{tang2024crsdiff,
  title={CRS-Diff: Controllable Remote Sensing Image Generation with Diffusion Model},
  author={Tang, Datao and Li, Wenyuan and others},
  journal={TGRS},
  year={2024}
}



@inproceedings{diffusionset2024,
	title = {{DiffusionSat}: {A} {Generative} {Foundation} {Model} for {Satellite} {Imagery}},
	shorttitle = {{DiffusionSat}},
  publisher = {ICLR},
	url = {https://openreview.net/forum?id=I5webNFDgQ},
	abstract = {Diffusion models have achieved state-of-the-art results on many modalities including images, speech, and video. However, existing models are not tailored to support remote sensing data, which is widely used in important applications including environmental monitoring and crop-yield prediction. Satellite images are significantly different from natural images -- they can be multi-spectral, irregularly sampled across time -- and existing diffusion models trained on images from the Web do not support them. Furthermore, remote sensing data is inherently spatio-temporal, requiring conditional generation tasks not supported by traditional methods based on captions or images. In this paper, we present DiffusionSat, to date the largest generative foundation model trained on a collection of publicly available large, high-resolution remote sensing datasets . As text-based captions are sparsely available for satellite images, we incorporate the associated metadata such as geolocation as conditioning information. Our method produces realistic samples and can be used to solve multiple generative tasks including temporal generation, multi-spectral superrresolution and in-painting. Our method outperforms previous state-of-the-art methods for satellite image generation and is the first large-scale \_generative\_ foundation model for satellite imagery. The project website can be found here: https://samar-khanna.github.io/DiffusionSat/},
	language = {en},
	urldate = {2024-08-30},
	author = {Khanna, Samar and Liu, Patrick and Zhou, Linqi and Meng, Chenlin and Rombach, Robin and Burke, Marshall and Lobell, David B. and Ermon, Stefano},
	month = oct,
	year = {2024},
	file = {Full Text PDF:D\:\\Zotero\\storage(sakura)\\storage\\DXUAT5XH\\Khanna et al. - 2023 - DiffusionSat A Generative Foundation Model for Satellite Imagery.pdf:application/pdf},
}



@misc{CVPR2023Tutorial,
	title = {{CVPR} 2023 {Tutorial} {Denoising} {Diffusion}-based {Generative} {Modeling}: {Foundations} and {Applications}},
	author={Vahdat，Arash and Song, Jiaming  and  Meng, Chenlin},
  shorttitle = {Denoising {Diffusion}-based {Generative} {Modeling}},
	url = {https://cvpr2023-tutorial-diffusion-models.github.io},
	abstract = {Tutorial in Conjunction with CVPR 2023},
  year={2023},
	urldate = {2024-10-16},
	file = {cvpr2023-diffusion-tutorial-part-1:D\:\\Zotero\\storage(sakura)\\storage\\FLNG7CND\\cvpr2023-diffusion-tutorial-part-1.pdf:application/pdf;cvpr2023-diffusion-tutorial-part-2:D\:\\Zotero\\storage(sakura)\\storage\\MCPB39KM\\cvpr2023-diffusion-tutorial-part-2.pdf:application/pdf;cvpr2023-diffusion-tutorial-part-3:D\:\\Zotero\\storage(sakura)\\storage\\ACF366JF\\cvpr2023-diffusion-tutorial-part-3.pdf:application/pdf},
}


@article{text2earth2025,
	title = {{Text2Earth}: {Unlocking} text-driven remote sensing image generation with a global-scale dataset and a foundation model},
	issn = {2168-6831},
	shorttitle = {{Text2Earth}},
	url = {https://ieeexplore.ieee.org/document/10988859},
	doi = {10.1109/MGRS.2025.3560455},
	abstract = {Recently, generative foundation models (GFMs) have significantly advanced large-scale text-driven natural image generation and become a prominent research trend across various vertical domains. However, in the remote sensing field, there is still a lack of research on large-scale text-to-image (text2image) generation technology. Existing remote sensing image‒text datasets are small in scale and confined to specific geographic areas and scene types. Besides, existing text2image methods have struggled to achieve global-scale, multiresolution controllability, and unbounded image generation. To address these challenges, this article presents two key contributions: the Git-10M dataset and the Text2Earth foundation model. Git-10M is a global-scale image‒text dataset consisting of 10.5 million image‒text pairs, five times larger than the previous largest one. The dataset covers a wide range of geographic scenes and contains essential geospatial metadata, significantly surpassing existing datasets in both size and diversity. Building on Git-10M, we propose Text2Earth, a 1.3 billion-parameter GFM based on the diffusion framework to model global-scale remote sensing scenes. Text2Earth integrates a resolution guidance mechanism, enabling users to specify image resolutions. A dynamic condition adaptation (DCA) strategy is proposed for training and inference to improve image generation quality. Text2Earth not only excels in zero-shot text2image generation but also demonstrates robust generalization and flexibility across multiple tasks, including unbounded scene construction, image editing, and cross-modal image generation. This robust capability surpasses previous models restricted to basic fixed sizes and limited scene types. On the previous text2image benchmark dataset, Text2Earth outperforms previous models, with a significantly improved +26.23 Fréchet inception distance (FID) score and +20.95\% zero-shot classification overall accuracy (Cls-OA) metric. Our project page is https://chen-yang-liu.github.io/Text2Earth/.},
	urldate = {2025-05-10},
	journal = {GRSM},
	author = {Liu, Chenyang and Chen, Keyan and Zhao, Rui and Zou, Zhengxia and Shi, Zhenwei},
	year = {2025},
	keywords = {Diffusion models, Foundation models, Image resolution, Image synthesis, Metadata, Noise reduction, Remote sensing, Spatial resolution, Training, Visualization},
	pages = {2--23},
	file = {Full Text PDF:D\:\\Zotero\\storage(sakura)\\storage\\CV9HYHCJ\\Liu et al. - 2025 - Text2Earth Unlocking text-driven remote sensing image generation with a global-scale dataset and a.pdf:application/pdf},
}

@inproceedings{tokerSatSynthAugmentingImageMask2024,
  title = {{{SatSynth}}: {{Augmenting Image-Mask Pairs}} through {{Diffusion Models}} for {{Aerial Semantic Segmentation}}},
  shorttitle = {{{SatSynth}}},
  publisher={CVPR},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Toker, Aysim and Eisenberger, Marvin and Cremers, Daniel and {Leal-Taix{\'e}}, Laura},
  year = {2024},
  pages = {27695--27705},
  urldate = {2024-11-21},
  file = {D\:\\ZoteroLib\\storage\\LTNUQYNE\\Toker_SatSynth_Augmenting_Image-Mask_CVPR_2024_supplemental.pdf;D\:\\ZoteroLib\\storage\\Z83J4BG2\\Toker et al. - 2024 - SatSynth Augmenting Image-Mask Pairs through Diffusion Models for Aerial Semantic Segmentation.pdf}
}


@inproceedings{heSYNTHETICDATAGENERATIVE2022,
  title = {Is synthetic data from generative models ready for image recognition?},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {He, Ruifei and Sun, Shuyang and Yu, Xin and Xue, Chuhui and Zhang, Wenqing and Torr, Philip and Bai, Song and Qi, Xiaojuan},
  year = {2023},
  publisher = {ICLR},
  month = sep,
  urldate = {2024-09-01},
  abstract = {Recent text-to-image generation models have shown promising results in generating high-fidelity photo-realistic images. Though the results are astonishing to human eyes, how applicable these generated images are for recognition tasks remains under-explored. In this work, we extensively study whether and how synthetic images generated from state-of-the-art text-to-image generation models can be used for image recognition tasks, and focus on two perspectives: synthetic data for improving classification models in the data-scare settings (i.e. zero-shot and few-shot), and synthetic data for large-scale model pre-training for transfer learning. We showcase the powerfulness and shortcomings of synthetic data from existing generative models, and propose strategies for better applying synthetic data for recognition tasks. Code: https://github.com/CVMI-Lab/SyntheticData.},
  file = {D\:\\ZoteroLib\\storage\\AKJ2342C\\He et al. - 2022 - IS SYNTHETIC DATA FROM GENERATIVE MODELS READY FOR IMAGE RECOGNITION.pdf;D\:\\ZoteroLib\\storage\\QXK48PZH\\id220_supp_final.pdf}
}


@misc{dengPPTAdvancedNueralNetwork2024,
  author       = {Deng, Zhijie},
  title        = {{[CS7352]} Advanced Neural Network Theory and Application},
  publisher     ={SJTU Spring},
  year         = {2024},
  note         = {Qing Yuan Research Institute, Shanghai Jiao Tong University},
  url          = {https://thudzj.github.io/},
  language     = {en}
}

@inproceedings{Chai2025DistillationSupervisedCL,
  title={Distillation-Supervised Convolutional Low-Rank Adaptation for Efficient Image Super-Resolution},
  author={Xinning Chai and Yao Zhang and Yuxuan Zhang and Zhengxue Cheng and Yingsheng Qin and Yucai Yang and Li Song},
  year={2025},
  publisher = {CVPRW 2025},
  url={https://api.semanticscholar.org/CorpusID:277787382}
}

@article{liuRemoteCLIPVisionLanguage2024,
  title = {RemoteCLIP: A Vision Language Foundation Model for Remote Sensing},
  shorttitle = {RemoteCLIP},
  author = {Liu, Fan and Chen, Delong and Guan, Zhangqingyun and Zhou, Xiaocong and Zhu, Jiale and Ye, Qiaolin and Fu, Liyong and Zhou, Jun},
  year = {2024},
  journal = {TGRS},
  volume = {62},
  pages = {1--16},
  issn = {1558-0644},
  doi = {10.1109/TGRS.2024.3390838},
  urldate = {2024-08-13}
}



@inproceedings{radfordLearningTransferableVisual2021,
  title = {Learning Transferable Visual Models From Natural Language Supervision},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  pages = {8748--8763},
  publisher = {ICML},
  issn = {2640-3498},
  urldate = {2024-07-07}
}

@inproceedings{wangRealESRGANTrainingRealWorld2021b,
  title = {Real-ESRGAN: Training Real-World Blind Super-Resolution With Pure Synthetic Data},
  shorttitle = {Real-ESRGAN},
  author = {Wang, Xintao and Xie, Liangbin and Dong, Chao and Shan, Ying},
  year = {2021},
  pages = {1905--1914},
  publisher = {ICCV},
  urldate = {2025-06-06}
}

@misc{chenFaithDiffUnleashingDiffusion2024,
  title = {{{FaithDiff}}: {{Unleashing Diffusion Priors}} for {{Faithful Image Super-resolution}}},
  shorttitle = {{{FaithDiff}}},
  author = {Chen, Junyang and Pan, Jinshan and Dong, Jiangxin},
  year = {2025},
  month = nov,
  number = {arXiv:2411.18824},
  eprint = {2411.18824},
  primaryclass = {cs},
  publisher = {CVPR},
  doi = {10.48550/arXiv.2411.18824},
  urldate = {2025-06-06},
  abstract = {Faithful image super-resolution (SR) not only needs to recover images that appear realistic, similar to image generation tasks, but also requires that the restored images maintain fidelity and structural consistency with the input. To this end, we propose a simple and effective method, named FaithDiff, to fully harness the impressive power of latent diffusion models (LDMs) for faithful image SR. In contrast to existing diffusion-based SR methods that freeze the diffusion model pre-trained on high-quality images, we propose to unleash the diffusion prior to identify useful information and recover faithful structures. As there exists a significant gap between the features of degraded inputs and the noisy latent from the diffusion model, we then develop an effective alignment module to explore useful features from degraded inputs to align well with the diffusion process. Considering the indispensable roles and interplay of the encoder and diffusion model in LDMs, we jointly fine-tune them in a unified optimization framework, facilitating the encoder to extract useful features that coincide with diffusion process. Extensive experimental results demonstrate that FaithDiff outperforms state-of-the-art methods, providing high-quality and faithful SR results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\ZoteroLib\\storage\\XZ6C8G5Q\\Chen et al. - 2024 - FaithDiff Unleashing Diffusion Priors for Faithful Image Super-resolution.pdf;D\:\\ZoteroLib\\storage\\E4M9S6T7\\2411.html}
}

@article{yueEfficientDiffusionModel2025,
  title = {Efficient {{Diffusion Model}} for {{Image Restoration}} by {{Residual Shifting}}},
  author = {Yue, Zongsheng and Wang, Jianyi and Loy, Chen Change},
  year = {2025},
  month = jan,
  journal = {TPAMI},
  volume = {47},
  number = {1},
  pages = {116--130},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2024.3461721},
  urldate = {2025-06-06},
  abstract = {While diffusion-based image restoration (IR) methods have achieved remarkable success, they are still limited by the low inference speed attributed to the necessity of executing hundreds or even thousands of sampling steps. Existing acceleration sampling techniques, though seeking to expedite the process, inevitably sacrifice performance to some extent, resulting in over-blurry restored outcomes. To address this issue, this study proposes a novel and efficient diffusion model for IR that significantly reduces the required number of diffusion steps. Our method avoids the need for post-acceleration during inference, thereby avoiding the associated performance deterioration. Specifically, our proposed method establishes a Markov chain that facilitates the transitions between the high-quality and low-quality images by shifting their residuals, substantially improving the transition efficiency. A carefully formulated noise schedule is devised to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experimental evaluations demonstrate that the proposed method achieves superior or comparable performance to current state-of-the-art methods on four classical IR tasks, namely image super-resolution, image inpainting, blind face restoration, and image deblurring, even only with four sampling steps.},
  keywords = {Degradation,Diffusion models,Face restoration,image inpainting,Image restoration,image super-resolution,Image synthesis,Markov chain,Noise,noise schedule,Schedules,Superresolution},
  file = {D:\ZoteroLib\storage\KMMSW6PS\Yue et al. - 2025 - Efficient Diffusion Model for Image Restoration by Residual Shifting.pdf}
}

@article{wangExploitingDiffusionPrior2024,
  title = {Exploiting {{Diffusion Prior}} for {{Real-World Image Super-Resolution}}},
  author = {Wang, Jianyi and Yue, Zongsheng and Zhou, Shangchen and Chan, Kelvin C. K. and Loy, Chen Change},
  year = {2024},
  month = dec,
  journal = {IJCV},
  volume = {132},
  number = {12},
  pages = {5929--5949},
  issn = {1573-1405},
  doi = {10.1007/s11263-024-02168-7},
  urldate = {2025-06-06},
  abstract = {We present a novel approach to leverage prior knowledge encapsulated in pre-trained text-to-image diffusion models for blind super-resolution. Specifically, by employing our time-aware encoder, we can achieve promising restoration results without altering the pre-trained synthesis model, thereby preserving the generative prior and minimizing training cost. To remedy the loss of fidelity caused by the inherent stochasticity of diffusion models, we employ a controllable feature wrapping module that allows users to balance quality and fidelity by simply adjusting a scalar value during the inference process. Moreover, we develop a progressive aggregation sampling strategy to overcome the fixed-size constraints of pre-trained diffusion models, enabling adaptation to resolutions of any size. A comprehensive evaluation of our method using both synthetic and real-world benchmarks demonstrates its superiority over current state-of-the-art approaches. Code and models are available at https://github.com/IceClear/StableSR.},
  langid = {english},
  keywords = {Applications of Brownian Motion and Diffusion Theory,Diffusion  Processes and Stochastic Analysis on  Manifolds,Diffusion models,Diffusion Process,Diffusion Tensor Imaging,Generative prior,Image Processing,Image restoration,Super-resolution,Super-Resolution Microscopy},
  file = {D:\ZoteroLib\storage\7PKMLKRG\Wang et al. - 2024 - Exploiting Diffusion Prior for Real-World Image Super-Resolution.pdf}
}

@article{devriesImprovedRegularizationConvolutional2017,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1708.04552},
  year={2017}
}

@inproceedings{yunCutmixRegularizationStrategy2019,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6023--6032},
  year={2019},
  publisher={ICCV}
}

@inproceedings{ghiasiSimpleCopyPasteStrong2021,
  title={Simple copy-paste is a strong data augmentation method for instance segmentation},
  author={Ghiasi, Golnaz and Cui, Yin and Srinivas, Aravind and Qian, Rui and Lin, Tsung-Yi and Cubuk, Ekin D and Le, Quoc V and Zoph, Barret},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2918--2928},
  year={2021},
  publisher={CVPR}
}

@inproceedings{waqas2019isaid,
  title={isaid: A large-scale dataset for instance segmentation in aerial images},
  author={Waqas Zamir, Syed and Arora, Aditya and Gupta, Akshita and Khan, Salman and Sun, Guolei and Shahbaz Khan, Fahad and Zhu, Fan and Shao, Ling and Xia, Gui-Song and Bai, Xiang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={28--37},
  year={2019},
  publisher = {CVPRW},
}


@article{lu2017exploring,
	title = {Exploring {Models} and {Data} for {Remote} {Sensing} {Image} {Caption} {Generation}},
	volume = {56},
	issn = {1558-0644},
	url = {https://ieeexplore.ieee.org/abstract/document/8240966},
	doi = {10.1109/TGRS.2017.2776321},
	abstract = {Inspired by recent development of artificial satellite, remote sensing images have attracted extensive attention. Recently, notable progress has been made in scene classification and target detection. However, it is still not clear how to describe the remote sensing image content with accurate and concise sentences. In this paper, we investigate to describe the remote sensing images with accurate and flexible sentences. First, some annotated instructions are presented to better describe the remote sensing images considering the special characteristics of remote sensing images. Second, in order to exhaustively exploit the contents of remote sensing images, a large-scale aerial image data set is constructed for remote sensing image caption. Finally, a comprehensive review is presented on the proposed data set to fully advance the task of remote sensing caption. Extensive experiments on the proposed data set demonstrate that the content of the remote sensing image can be completely described by generating language descriptions. The data set is available at https://github.com/201528014227051/RSICD\_optimal.},
	number = {4},
	urldate = {2024-09-09},
	journal = {TGRS},
	author = {Lu, Xiaoqiang and Wang, Binqiang and Zheng, Xiangtao and Li, Xuelong},
	month = apr,
	year = {2018},
	note = {RSICD},
	keywords = {Computer vision, Feature extraction, Semantics, Remote sensing, remote sensing image, Recurrent neural networks, Image representation, Image captioning, semantic understanding},
	pages = {2183--2195},
	file = {IEEE Xplore Abstract Record:D\:\\Zotero\\storage(sakura)\\storage\\EY2UQVWV\\8240966.html:text/html;IEEE Xplore Full Text PDF:D\:\\Zotero\\storage(sakura)\\storage\\H67238VU\\Lu et al. - 2018 - Exploring Models and Data for Remote Sensing Image Caption Generation.pdf:application/pdf},
}


@article{congFunctionalMapWorld2022,
	title = {Functional {Map} of the {World} - {Sentinel}-2 corresponding images},
	url = {https://purl.stanford.edu/vg497cb6002},
	doi = {10.25740/vg497cb6002},
	abstract = {The Functional Map of the World - Sentinel-2 corresponding images (fMoW-Sentinel) dataset consists of image time series collected by the Sentinel-2 satellite, corresponding to locations from the Fu...},
	language = {en},
	urldate = {2024-06-29},
	author = {Cong, Yezhen and Khanna, Samar and Meng, Chenlin and Liu, Patrick and Rozi, Erik and He, Yutong and Burke, Marshall and Lobell, David B. and Ermon, Stefano},
	year = {2022},
	file = {Snapshot:D\:\\Zotero\\storage(sakura)\\storage\\R3V2PZ43\\vg497cb6002.html:text/html},
}


@inproceedings{christieFunctionalMapWorld2018,
	title = {Functional {Map} of the {World}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Christie_Functional_Map_of_CVPR_2018_paper.html},
	urldate = {2024-06-25},
  publisher = {CVPR},
	author = {Christie, Gordon and Fendley, Neil and Wilson, James and Mukherjee, Ryan},
	year = {2018},
	note = {https://github.com/fMoW/dataset},
	pages = {6172--6180},
	file = {Christie et al_2018_Functional Map of the World.pdf:D\:\\Zotero\\storage(sakura)\\storage\\VTS3XRJS\\Christie et al_2018_Functional Map of the World.pdf:application/pdf;Christie et al. - Supplementary Material Functional Map of the Worl.pdf:D\:\\Zotero\\storage(sakura)\\storage\\39FUI7KE\\Christie et al. - Supplementary Material Functional Map of the Worl.pdf:application/pdf},
}


@article{hu2023rsgpt,
	title = {{RSGPT}: {A} remote sensing vision language model and benchmark},
	volume = {224},
	issn = {0924-2716},
	shorttitle = {{RSGPT}},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271625001352},
	doi = {10.1016/j.isprsjprs.2025.03.028},
	abstract = {The emergence of large-scale Large Language Models (LLMs), with GPT-4 as a prominent example, has significantly propelled the rapid advancement of Artificial General Intelligence (AGI) and sparked the revolution of Artificial Intelligence 2.0. In the realm of remote sensing, there is a growing interest in developing large vision language models (VLMs) specifically tailored for data analysis in this domain. However, current research predominantly revolves around visual recognition tasks, lacking comprehensive, high-quality image–text datasets that are aligned and suitable for training large VLMs, which poses significant challenges to effectively training such models for remote sensing applications. In computer vision, recent research has demonstrated that fine-tuning large vision language models on small-scale, high-quality datasets can yield impressive performance in visual and language understanding. These results are comparable to state-of-the-art VLMs trained from scratch on massive amounts of data, such as GPT-4. Inspired by this captivating idea, in this work, we build a high-quality Remote Sensing Image Captioning dataset (RSICap) that facilitates the development of large VLMs in the remote sensing field. Unlike previous remote sensing datasets that either employ model-generated captions or short descriptions, RSICap comprises 2,585 human-annotated captions with rich and high-quality information. This dataset offers detailed descriptions for each image, encompassing scene descriptions (e.g., residential area, airport, or farmland) as well as object information (e.g., color, shape, quantity, absolute position, etc.). To facilitate the evaluation of VLMs in the field of remote sensing, we also provide a benchmark evaluation dataset called RSIEval. This dataset consists of human-annotated captions and visual question–answer pairs, allowing for a comprehensive assessment of VLMs in the context of remote sensing. We are actively engaged in expanding the scale of these two datasets to cover a broader spectrum of remote sensing image understanding tasks, further enhancing their utility and applicability. Our dataset and codes will be released at https://github.com/Lavender105/RSGPT.},
	urldate = {2025-06-01},
	journal = {ISPRS},
	author = {Hu, Yuan and Yuan, Jianlong and Wen, Congcong and Lu, Xiaonan and Liu, Yu and Li, Xiang},
	month = jun,
	year = {2025},
	keywords = {Multi-modal large language model (MLLM), Remote sensing (RS), Vision language model (VLM)},
	pages = {272--286},
	file = {PDF:D\:\\Zotero\\storage(sakura)\\storage\\6Y94GBT4\\Hu et al. - 2025 - RSGPT A remote sensing vision language model and benchmark.pdf:application/pdf},
}


@inproceedings{qu2016deep,
  title={Deep semantic understanding of high resolution remote sensing image},
  author={Qu, Bo and Li, Xuelong and Tao, Dacheng and Lu, Xiaoqiang},
  booktitle={2016 International Conference on Computer, Information and Telecommunication Systems (CITS)},
  pages={1--5},
  year={2016},
  organization={IEEE},
  publisher = {CITS},
}
@article{mengConditionalDiffusionModel2024,
  title = {A {{Conditional Diffusion Model With Fast Sampling Strategy}} for {{Remote Sensing Image Super-Resolution}}},
  author = {Meng, Fanen and Chen, Yijun and Jing, Haoyu and Zhang, Laifu and Yan, Yiming and Ren, Yingchao and Wu, Sensen and Feng, Tian and Liu, Renyi and Du, Zhenhong},
  year = {2024},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {62},
  pages = {1--16},
  issn = {1558-0644},
  doi = {10.1109/TGRS.2024.3458009},
  urldate = {2025-06-09},
  abstract = {Conventional deep learning-based methods for single remote sensing image super-resolution (SRSISR) have made remarkable progress. However, the super-resolution (SR) outputs of these methods are yet to become sufficiently satisfactory in visual quality. Recent diffusion model-based generative deep learning models are capable to enhance the visual quality of output images, but this capability is limited due to their sampling efficiency. In this article, we propose FastDiffSR, an SRSISR method based on a conditional diffusion model. Specifically, we devise a novel sampling strategy to reduce the number of sampling steps required by the diffusion model while ensuring the sampling quality. Meanwhile, the residual image is adopted to reduce computational costs, demonstrating that integrating channel attention and spatial attention begets a further improvement in the visual quality of output images. Compared to the state-of-the-art (SOTA) convolutional neural network (CNN)-based, GAN-based, and Transformer-based SR methods, our FastDiffSR improves the learned perceptual image patch similarity (LPIPS) by 0.1--0.2 and achieves better visual results in some real-world scenes. Compared with existing diffusion-based SR methods, our FastDiffSR achieves significant improvements in pixel-level evaluation metric peak signal-noise ratio (PSNR) while having smaller model parameters and obtaining better SR results on Vaihingen data with faster inference time by 2.8--28 times, showing excellent generalization ability and time efficiency. Our code will be open source at https://github.com/Meng-333/FastDiffSR.},
  keywords = {Computational modeling,Conditional diffusion model,deep learning,Diffusion models,generative models,remote sensing,Remote sensing,super-resolution (SR),Superresolution,Training,Transformers,Visualization},
  file = {D:\ZoteroLib\storage\U3SEYGG7\Meng et al. - 2024 - A Conditional Diffusion Model With Fast Sampling Strategy for Remote Sensing Image Super-Resolution.pdf}
}

@inproceedings{ucm2010,
author = {Yang, Yi and Newsam, Shawn},
title = {Bag-of-visual-words and spatial extensions for land-use classification},
year = {2010},
isbn = {9781450304283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1869790.1869829},
doi = {10.1145/1869790.1869829},
abstract = {We investigate bag-of-visual-words (BOVW) approaches to land-use classification in high-resolution overhead imagery. We consider a standard non-spatial representation in which the frequencies but not the locations of quantized image features are used to discriminate between classes analogous to how words are used for text document classification without regard to their order of occurrence. We also consider two spatial extensions, the established spatial pyramid match kernel which considers the absolute spatial arrangement of the image features, as well as a novel method which we term the spatial co-occurrence kernel that considers the relative arrangement. These extensions are motivated by the importance of spatial structure in geographic data.The methods are evaluated using a large ground truth image dataset of 21 land-use classes. In addition to comparisons with standard approaches, we perform extensive evaluation of different configurations such as the size of the visual dictionaries used to derive the BOVW representations and the scale at which the spatial relationships are considered.We show that even though BOVW approaches do not necessarily perform better than the best standard approaches overall, they represent a robust alternative that is more effective for certain land-use classes. We also show that extending the BOVW approach with our proposed spatial co-occurrence kernel consistently improves performance.},
booktitle = {Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {270–279},
numpages = {10},
keywords = {bag-of-visual-words, land-use classification, local invariant features},
location = {San Jose, California},
series = {GIS '10}
}

@article{ROTTENSTEINER2014256,
title = {Results of the ISPRS benchmark on urban object detection and 3D building reconstruction},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {93},
pages = {256-271},
year = {2014},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2013.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0924271613002268},
author = {Franz Rottensteiner and Gunho Sohn and Markus Gerke and Jan Dirk Wegner and Uwe Breitkopf and Jaewook Jung},
keywords = {Automatic object extraction, 3D building reconstruction, Aerial imagery, Laser scanning, Evaluation, Benchmarking test},
abstract = {For more than two decades, many efforts have been made to develop methods for extracting urban objects from data acquired by airborne sensors. In order to make the results of such algorithms more comparable, benchmarking data sets are of paramount importance. Such a data set, consisting of airborne image and laserscanner data, has been made available to the scientific community by ISPRS WGIII/4. Researchers were encouraged to submit their results of urban object detection and 3D building reconstruction, which were evaluated based on reference data. This paper presents the outcomes of the evaluation for building detection, tree detection, and 3D building reconstruction. The results achieved by different methods are compared and analysed to identify promising strategies for automatic urban object extraction from current airborne sensor data, but also common problems of state-of-the-art methods.}
}

@inproceedings{postdam,
  title = {Fully {{Convolutional Networks}} for {{Dense Semantic Labelling}} of {{High-Resolution Aerial Imagery}}},
  author = {Sherrah, Jamie},
  year = {2016},
  month = jun,
  number = {arXiv:1606.02585},
  eprint = {1606.02585},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1606.02585},
  urldate = {2025-06-09},
  abstract = {The trend towards higher resolution remote sensing imagery facilitates a transition from land-use classification to object-level scene understanding. Rather than relying purely on spectral content, appearance-based image features come into play. In this work, deep convolutional neural networks (CNNs) are applied to semantic labelling of high-resolution remote sensing data. Recent advances in fully convolutional networks (FCNs) are adapted to overhead data and shown to be as effective as in other domains. A full-resolution labelling is inferred using a deep FCN with no downsampling, obviating the need for deconvolution or interpolation. To make better use of image features, a pre-trained CNN is fine-tuned on remote sensing data in a hybrid network context, resulting in superior results compared to a network trained from scratch. The proposed approach is applied to the problem of labelling high-resolution aerial imagery, where fine boundary detail is important. The dense labelling yields state-of-the-art accuracy for the ISPRS Vaihingen and Potsdam benchmark data sets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}
